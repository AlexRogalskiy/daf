
kafka.bootstrap.servers = "localhost:9092"
kafka.group.id = "daf-iot-manager"
kafka.topic = "daf-iot-events"

hdfs.path = "testKafka"

#Kudu Parameters
kudu.master.addresses = "localhost:64308"
kudu.events.table.name = "Events"
kudu.offsets.table.name = "Offsets"
kudu.events.table.numberOfBuckets = "8"
kudu.offsets.table.numberOfBuckets = "8"

#Spark Parameters
batch.duration = 1000
#spark.yarn.jars = "local:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*"
spark.serializer = "org.apache.spark.serializer.KryoSerializer"
spark.io.compression.codec = "lzf"
spark.speculation = "false"
spark.shuffle.manager= "sort"
spark.shuffle.service.enabled = "true"
spark.dynamicAllocation.enabled = "true"
spark.dynamicAllocation.minExecutors = "4"
spark.dynamicAllocation.initialExecutors = "4"
spark.executor.cores = "2"
spark.executor.memory = "2048m"
spark.streaming.backpressure.enabled = "true"
spark.streaming.kafka.maxRatePerPartition = "500"
#spark.driver.extraClassPath = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.12.0.jar"
#spark.executor.extraClassPath = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.12.0.jar"
#spark.executor.extraJavaOptions = "-Djava.security.auth.login.config=/tmp/jaas.conf"

